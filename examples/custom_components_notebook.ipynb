{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Building Custom RAG Components from Scratch\n",
    "\n",
    "This notebook demonstrates how to create your own custom RAG pipeline components by implementing the base class interfaces. This is useful when you want to:\n",
    "\n",
    "- Integrate a custom embedding model or API\n",
    "- Use a different vector database backend\n",
    "- Implement specialized chunking strategies\n",
    "- Create custom reranking logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "# Base classes to extend\n",
    "from rag_search.parameter_impls.embedding_impls.embedding_base import BaseEmbedding\n",
    "from rag_search.parameter_impls.storage_impls.storage_base import BaseStorage\n",
    "\n",
    "# RAGClient and built-in components we'll mix with our custom ones\n",
    "from rag_search.rag_client import RAGClient\n",
    "from rag_search.parameter_impls.ingestion_impls import SimpleIngestion\n",
    "from rag_search.parameter_impls.chunking_impls import SlidingWindowChunking\n",
    "from rag_search.parameter_impls.retriever_impls import SimpleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-embedding-intro",
   "metadata": {},
   "source": [
    "## 1. Custom Embedding: Wrapping Sentence Transformers\n",
    "\n",
    "Let's create a custom embedding component that wraps the `sentence-transformers` library directly. This shows how you can integrate any embedding model or API.\n",
    "\n",
    "**Required methods:**\n",
    "- `embed_text(text: str) -> List[float]` - Embed a single text\n",
    "- `embed_batch(texts: List[str]) -> List[List[float]]` - Embed multiple texts efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-custom-embedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agnimagarwal/miniforge3/envs/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n",
      "\n",
      "Embedding for: 'This is a test sentence for embedding.'\n",
      "Vector length: 384\n",
      "First 5 values: [0.027824116870760918, 0.0017025501001626253, 0.08005548268556595, 0.046662840992212296, 0.03852203115820885]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class CustomSentenceTransformerEmbedding(BaseEmbedding):\n",
    "    \"\"\"Custom embedding implementation using sentence-transformers directly.\n",
    "    \n",
    "    This demonstrates how to wrap any embedding model or API to work\n",
    "    with the RAG pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\"):\n",
    "        \"\"\"Initialize with a sentence-transformers model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence-transformers model to use.\n",
    "                       See https://www.sbert.net/docs/pretrained_models.html\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"Loaded embedding model: {model_name}\")\n",
    "        print(f\"Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single text string.\"\"\"\n",
    "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed multiple texts efficiently in a batch.\"\"\"\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "# Test the custom embedding\n",
    "custom_embedding = CustomSentenceTransformerEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "test_text = \"This is a test sentence for embedding.\"\n",
    "embedding = custom_embedding.embed_text(test_text)\n",
    "print(f\"\\nEmbedding for: '{test_text}'\")\n",
    "print(f\"Vector length: {len(embedding)}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-storage-intro",
   "metadata": {},
   "source": [
    "## 2. Custom Storage: FAISS Vector Store\n",
    "\n",
    "Now let's create a custom storage backend using [FAISS](https://github.com/facebookresearch/faiss) for efficient similarity search. FAISS is optimized for fast nearest-neighbor search on large vector datasets.\n",
    "\n",
    "**Required methods:**\n",
    "- `add_documents(chunks, embeddings, metadata)` - Store documents with their embeddings\n",
    "- `search(query_embedding, top_k, ...)` - Find similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-custom-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS storage initialized\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "class FAISSStorage(BaseStorage):\n",
    "    \"\"\"Custom storage backend using FAISS for efficient similarity search.\n",
    "    \n",
    "    This demonstrates how to integrate a custom vector database or\n",
    "    search backend with the RAG pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = False):\n",
    "        \"\"\"Initialize FAISS storage.\n",
    "        \n",
    "        Args:\n",
    "            use_gpu: Whether to use GPU acceleration (requires faiss-gpu)\n",
    "        \"\"\"\n",
    "        self.use_gpu = use_gpu\n",
    "        self.index = None\n",
    "        self.dimension = None\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        chunks: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadata: Optional[List[Dict[str, Any]]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add documents to the FAISS index.\"\"\"\n",
    "        if not chunks:\n",
    "            return\n",
    "        \n",
    "        # Convert embeddings to numpy array\n",
    "        vectors = np.array(embeddings, dtype=np.float32)\n",
    "        \n",
    "        # Initialize index on first call\n",
    "        if self.index is None:\n",
    "            self.dimension = vectors.shape[1]\n",
    "            # Using IndexFlatIP for inner product (cosine similarity with normalized vectors)\n",
    "            self.index = faiss.IndexFlatIP(self.dimension)\n",
    "            if self.use_gpu and faiss.get_num_gpus() > 0:\n",
    "                self.index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, self.index)\n",
    "        \n",
    "        # Normalize vectors for cosine similarity\n",
    "        faiss.normalize_L2(vectors)\n",
    "        \n",
    "        # Add vectors to index\n",
    "        self.index.add(vectors)\n",
    "        \n",
    "        # Store document metadata\n",
    "        start_idx = len(self.documents)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = {\n",
    "                \"text\": chunk,\n",
    "                \"chunk_id\": start_idx + i,\n",
    "            }\n",
    "            if metadata and i < len(metadata):\n",
    "                doc.update(metadata[i])\n",
    "            self.documents.append(doc)\n",
    "        \n",
    "        print(f\"Added {len(chunks)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query_embedding: List[float],\n",
    "        top_k: int = 5,\n",
    "        similarity_func: str = \"cosine\",\n",
    "        filter_metadata: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar documents using FAISS.\"\"\"\n",
    "        if self.index is None or len(self.documents) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Prepare query vector\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vector, min(top_k, len(self.documents)))\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < 0:  # FAISS returns -1 for empty slots\n",
    "                continue\n",
    "            \n",
    "            doc = self.documents[idx].copy()\n",
    "            doc[\"score\"] = float(score)\n",
    "            \n",
    "            # Apply metadata filter if provided\n",
    "            if filter_metadata:\n",
    "                if all(doc.get(k) == v for k, v in filter_metadata.items()):\n",
    "                    results.append(doc)\n",
    "            else:\n",
    "                results.append(doc)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all documents from storage.\"\"\"\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def count(self) -> int:\n",
    "        \"\"\"Return the number of documents in storage.\"\"\"\n",
    "        return len(self.documents)\n",
    "\n",
    "\n",
    "# Test the custom storage\n",
    "custom_storage = FAISSStorage()\n",
    "print(f\"FAISS storage initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-corpus-intro",
   "metadata": {},
   "source": [
    "## 3. Sample Corpus\n",
    "\n",
    "Let's define a sample document corpus for testing our custom components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 5 documents\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"doc_id\": \"doc_1\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy. It works by first retrieving relevant documents from a knowledge base, then using those documents as context for a language model to generate responses.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_2\",\n",
    "        \"text\": \"Vector databases store embeddings and enable efficient similarity search for retrieval systems. They use specialized indexing structures like HNSW or IVF to find nearest neighbors in high-dimensional spaces quickly.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_3\",\n",
    "        \"text\": \"Chunking strategies such as sliding windows affect recall and precision in RAG pipelines. Smaller chunks provide more precise retrieval but may lose context, while larger chunks preserve context but may include irrelevant information.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_4\",\n",
    "        \"text\": \"Embedding models map text into dense vector representations used for semantic search. Models like BERT, sentence-transformers, and OpenAI embeddings capture semantic meaning, allowing retrieval based on meaning rather than exact keyword matches.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_5\",\n",
    "        \"text\": \"FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Corpus contains {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pipeline-intro",
   "metadata": {},
   "source": [
    "## 4. Building the Pipeline with Custom Components\n",
    "\n",
    "Now we'll wire up our custom components with the built-in ones to create a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n",
      "\n",
      "RAGClient created with custom embedding and storage!\n"
     ]
    }
   ],
   "source": [
    "# Create fresh instances of our custom components\n",
    "custom_embedding = CustomSentenceTransformerEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "custom_storage = FAISSStorage()\n",
    "\n",
    "# Create a retriever using our custom components\n",
    "custom_retriever = SimpleRetriever(\n",
    "    storage=custom_storage,\n",
    "    embedding=custom_embedding,\n",
    "    top_k=3,\n",
    "    reranker=None  # No reranking for this example\n",
    ")\n",
    "\n",
    "# Build the RAGClient mixing custom and built-in components\n",
    "client = RAGClient(\n",
    "    ingestion=SimpleIngestion(),           # Built-in: simple text ingestion\n",
    "    chunking=SlidingWindowChunking(        # Built-in: sliding window chunking\n",
    "        chunk_size=256,\n",
    "        chunk_overlap=50\n",
    "    ),\n",
    "    embedding=custom_embedding,             # Custom: our sentence-transformers wrapper\n",
    "    storage=custom_storage,                 # Custom: our FAISS storage\n",
    "    retriever=custom_retriever,             # Uses our custom embedding + storage\n",
    ")\n",
    "\n",
    "print(\"\\nRAGClient created with custom embedding and storage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-upload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading documents...\n",
      "\n",
      "Added 10 documents. Total: 10\n",
      "\n",
      "Total chunks in storage: 10\n"
     ]
    }
   ],
   "source": [
    "# Upload documents to the pipeline\n",
    "print(\"Uploading documents...\\n\")\n",
    "client.upload_documents(documents)\n",
    "print(f\"\\nTotal chunks in storage: {custom_storage.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-test-intro",
   "metadata": {},
   "source": [
    "## 5. Test Retrieval\n",
    "\n",
    "Let's test our custom pipeline with some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG and how does it work?\n",
      "------------------------------------------------------------\n",
      "  1. [Score: 0.3457] (doc_1) Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve ...\n",
      "  2. [Score: 0.3354] (doc_3) Chunking strategies such as sliding windows affect recall and precision in RAG pipelines. Smaller ch...\n",
      "  3. [Score: 0.1150] (doc_4) ning rather than exact keyword matches.\n",
      "\n",
      "Query: How do vector databases enable fast search?\n",
      "------------------------------------------------------------\n",
      "  1. [Score: 0.6646] (doc_2) Vector databases store embeddings and enable efficient similarity search for retrieval systems. They...\n",
      "  2. [Score: 0.4354] (doc_5) FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of...\n",
      "  3. [Score: 0.3953] (doc_4) Embedding models map text into dense vector representations used for semantic search. Models like BE...\n",
      "\n",
      "Query: What is FAISS used for?\n",
      "------------------------------------------------------------\n",
      "  1. [Score: 0.3849] (doc_5) FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of...\n",
      "  2. [Score: 0.2091] (doc_3) lude irrelevant information.\n",
      "  3. [Score: 0.1667] (doc_4) ning rather than exact keyword matches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is RAG and how does it work?\",\n",
    "    \"How do vector databases enable fast search?\",\n",
    "    \"What is FAISS used for?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = client.retrieve(query)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        score = result.get('score', 0)\n",
    "        text = result.get('text', '')[:100] + \"...\" if len(result.get('text', '')) > 100 else result.get('text', '')\n",
    "        doc_id = result.get('doc_id', 'N/A')\n",
    "        print(f\"  {i}. [Score: {score:.4f}] ({doc_id}) {text}\")\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
