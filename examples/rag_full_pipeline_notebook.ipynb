{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RAG Pipeline Optimization Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Setting up an unstructured document dataset\n",
    "2. Creating an evaluation set with ground truth relevance judgments\n",
    "3. Testing multiple RAG pipeline configurations using instance-based component variations\n",
    "4. Evaluating retrieval relevance metrics (Precision@K, Recall@K, MRR)\n",
    "5. Selecting the best pipeline configuration\n",
    "6. Integrating the optimized pipeline with LangChain for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<your_openai_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agnimagarwal/miniforge3/envs/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "from rag_search.rag_client import RAGClient\n",
    "from rag_search.experiment.pipeline import generate_client_variations, build_client, artifact_from_client\n",
    "from rag_search.experiment.artifact.rag_artifact import RAGArtifact\n",
    "\n",
    "# Ingestion implementations\n",
    "from rag_search.parameter_impls.ingestion_impls import (\n",
    "    SimpleIngestion,\n",
    ")\n",
    "\n",
    "# Chunking implementations\n",
    "from rag_search.parameter_impls.chunking_impls import (\n",
    "    SlidingWindowChunking,\n",
    "    LangChainRecursiveChunking,\n",
    ")\n",
    "\n",
    "# Embedding implementations\n",
    "from rag_search.parameter_impls.embedding_impls import (\n",
    "    HuggingFaceEmbedding,\n",
    ")\n",
    "\n",
    "# Storage implementations\n",
    "from rag_search.parameter_impls.storage_impls import (\n",
    "    SimpleStorage,\n",
    ")\n",
    "\n",
    "# Retriever implementations\n",
    "from rag_search.parameter_impls.retriever_impls import (\n",
    "    SimpleRetriever,\n",
    ")\n",
    "\n",
    "# Optional reranker\n",
    "from rag_search.parameter_impls.reranking_impls import (\n",
    "    CrossEncoderReranker,\n",
    ")\n",
    "\n",
    "# LangChain imports for Q&A (consume RAGArtifact as a retriever)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Setup: Create Sample Document Dataset\n",
    "\n",
    "We'll create a sample document corpus about various topics for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define component variations to test\n",
    "ingestion_variants = [\n",
    "    SimpleIngestion(),\n",
    "]\n",
    "\n",
    "chunking_variants = [\n",
    "    SlidingWindowChunking(chunk_size=128, chunk_overlap=20),\n",
    "    SlidingWindowChunking(chunk_size=256, chunk_overlap=50),\n",
    "    SlidingWindowChunking(chunk_size=512, chunk_overlap=100),\n",
    "    LangChainRecursiveChunking(chunk_size=512, chunk_overlap=50),\n",
    "]\n",
    "\n",
    "embedding_variants = [\n",
    "    HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2'),\n",
    "]\n",
    "\n",
    "storage_variants = [\n",
    "    SimpleStorage(),\n",
    "]\n",
    "\n",
    "# Retriever variants (optionally with reranker)\n",
    "retriever_variants = [\n",
    "    SimpleRetriever(storage=storage_variants[0], embedding=embedding_variants[0], top_k=3, reranker=None),\n",
    "    SimpleRetriever(storage=storage_variants[0], embedding=embedding_variants[0], top_k=5, reranker=None),\n",
    "    SimpleRetriever(storage=storage_variants[0], embedding=embedding_variants[0], top_k=5, reranker=CrossEncoderReranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99185da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Define corpus documents\n",
    "# -----------------------------\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"doc_id\": \"doc_1\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_2\",\n",
    "        \"text\": \"Vector databases store embeddings and enable efficient similarity search for retrieval systems.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_3\",\n",
    "        \"text\": \"Chunking strategies such as sliding windows affect recall and precision in RAG pipelines.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_4\",\n",
    "        \"text\": \"Embedding models map text into dense vector representations used for semantic search.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Define evaluation dataset\n",
    "# -----------------------------\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"relevant_doc_ids\": [\"doc_1\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What do vector databases do?\",\n",
    "        \"relevant_doc_ids\": [\"doc_2\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does chunking affect RAG systems?\",\n",
    "        \"relevant_doc_ids\": [\"doc_3\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are embeddings used for?\",\n",
    "        \"relevant_doc_ids\": [\"doc_4\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metric helper functions\n",
    "# -----------------------------\n",
    "\n",
    "def calculate_precision_at_k(retrieved_ids, relevant_ids, k):\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    retrieved_k = retrieved_ids[:k]\n",
    "    if not retrieved_k:\n",
    "        return 0.0\n",
    "    hits = sum(1 for doc_id in retrieved_k if doc_id in relevant_ids)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(retrieved_ids, relevant_ids, k):\n",
    "    if not relevant_ids:\n",
    "        return 0.0\n",
    "    retrieved_k = retrieved_ids[:k]\n",
    "    hits = sum(1 for doc_id in retrieved_k if doc_id in relevant_ids)\n",
    "    return hits / len(relevant_ids)\n",
    "\n",
    "\n",
    "def calculate_mrr(retrieved_ids, relevant_ids):\n",
    "    for idx, doc_id in enumerate(retrieved_ids, start=1):\n",
    "        if doc_id in relevant_ids:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation Set with Ground Truth\n",
    "\n",
    "Define queries and their relevant documents for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 12 pipeline configurations...\n",
      "\n",
      "Config 1: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 2: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 3: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 1.0000\n",
      "\n",
      "Config 4: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 5: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 6: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "Config 7: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 8: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 9: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "Config 10: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 11: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "Config 12: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "Best Configuration: Config 3: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "Best MRR Score: 1.0000\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run experiments for each pipeline variation (output is a RAGArtifact per config)\n",
    "experiment_results = []\n",
    "\n",
    "configs = generate_client_variations(\n",
    "    ingestion_variants=ingestion_variants,\n",
    "    chunking_variants=chunking_variants,\n",
    "    embedding_variants=embedding_variants,\n",
    "    storage_variants=storage_variants,\n",
    "    retriever_variants=retriever_variants,\n",
    ")\n",
    "\n",
    "print(f\"Testing {len(configs)} pipeline configurations...\\n\")\n",
    "\n",
    "for cfg in configs:\n",
    "    storage = SimpleStorage()\n",
    "    embedding = cfg.embedding\n",
    "    retriever = SimpleRetriever(storage=storage, embedding=embedding, top_k=getattr(cfg.retriever, 'top_k', 5), reranker=getattr(cfg.retriever, 'reranker', None))\n",
    "    client = RAGClient(\n",
    "        ingestion=cfg.ingestion,\n",
    "        chunking=cfg.chunking,\n",
    "        embedding=embedding,\n",
    "        storage=storage,\n",
    "        retriever=retriever,\n",
    "    )\n",
    "\n",
    "    # Ingest corpus\n",
    "    client.upload_documents(documents)\n",
    "\n",
    "    # Evaluate retrieval over the evaluation dataset\n",
    "    metrics_list = []\n",
    "    for item in eval_dataset:\n",
    "        q = item['query']\n",
    "        relevant_ids = item['relevant_doc_ids']\n",
    "        results = client.retrieve(q)\n",
    "        retrieved_ids = [r.get('doc_id') for r in results]\n",
    "\n",
    "        metrics_list.append({\n",
    "            'precision@3': calculate_precision_at_k(retrieved_ids, relevant_ids, 3),\n",
    "            'precision@5': calculate_precision_at_k(retrieved_ids, relevant_ids, 5),\n",
    "            'recall@5': calculate_recall_at_k(retrieved_ids, relevant_ids, 5),\n",
    "            'mrr': calculate_mrr(retrieved_ids, relevant_ids),\n",
    "        })\n",
    "\n",
    "    avg_metrics = {k: sum(m[k] for m in metrics_list)/len(metrics_list) for k in metrics_list[0].keys()}\n",
    "\n",
    "    params = {\n",
    "        'ingestion': type(cfg.ingestion).__name__,\n",
    "        'chunking': type(cfg.chunking).__name__,\n",
    "        'embedding': type(embedding).__name__,\n",
    "        'storage': type(storage).__name__,\n",
    "        'retriever': type(retriever).__name__,\n",
    "        'top_k': getattr(retriever, 'top_k', None),\n",
    "        'reranker': type(getattr(retriever, 'reranker', None)).__name__ if getattr(retriever, 'reranker', None) else None,\n",
    "    }\n",
    "\n",
    "    artifact = artifact_from_client(client=client, experiment_params=params, metrics=avg_metrics)\n",
    "    experiment_results.append({\n",
    "        'config_name': cfg.name,\n",
    "        'artifact': artifact,\n",
    "        'metrics': avg_metrics,\n",
    "    })\n",
    "\n",
    "    print(cfg.name)\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find best configuration based on MRR\n",
    "best_result = max(experiment_results, key=lambda x: x['metrics']['mrr'])\n",
    "best_artifact: RAGArtifact = best_result['artifact']\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"Best Configuration: {best_result['config_name']}\")\n",
    "print(f\"Best MRR Score: {best_result['metrics']['mrr']:.4f}\")\n",
    "print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Metrics\n",
    "\n",
    "Implement retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.3333\n",
      "Recall@3: 1.0000\n",
      "MRR: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# The metrics helper functions are already defined in the cell above (99185da0)\n",
    "# Here's a summary of what they do:\n",
    "\n",
    "# calculate_precision_at_k: Measures what fraction of retrieved docs are relevant\n",
    "# calculate_recall_at_k: Measures what fraction of relevant docs were retrieved  \n",
    "# calculate_mrr: Mean Reciprocal Rank - measures how high the first relevant doc ranks\n",
    "\n",
    "# Example usage:\n",
    "retrieved = [\"doc_1\", \"doc_3\", \"doc_2\"]\n",
    "relevant = [\"doc_1\"]\n",
    "\n",
    "print(f\"Precision@3: {calculate_precision_at_k(retrieved, relevant, 3):.4f}\")\n",
    "print(f\"Recall@3: {calculate_recall_at_k(retrieved, relevant, 3):.4f}\")\n",
    "print(f\"MRR: {calculate_mrr(retrieved, relevant):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Define Pipeline Component Variations\n",
    "\n",
    "Create sets of component instances to test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion variants: 1\n",
      "Chunking variants: 4\n",
      "Embedding variants: 1\n",
      "Storage variants: 1\n",
      "Retriever variants: 3\n",
      "\n",
      "Total possible configurations: 12\n"
     ]
    }
   ],
   "source": [
    "# The component variations are already defined in cell-4 above:\n",
    "# - ingestion_variants: Different document ingestion strategies\n",
    "# - chunking_variants: Different chunking strategies (sliding window, recursive)\n",
    "# - embedding_variants: Different embedding models\n",
    "# - storage_variants: Different storage backends\n",
    "# - retriever_variants: Different retrieval configurations (top_k, reranker)\n",
    "\n",
    "# Print summary of variations\n",
    "print(f\"Ingestion variants: {len(ingestion_variants)}\")\n",
    "print(f\"Chunking variants: {len(chunking_variants)}\")\n",
    "print(f\"Embedding variants: {len(embedding_variants)}\")\n",
    "print(f\"Storage variants: {len(storage_variants)}\")\n",
    "print(f\"Retriever variants: {len(retriever_variants)}\")\n",
    "print(f\"\\nTotal possible configurations: {len(ingestion_variants) * len(chunking_variants) * len(retriever_variants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Run Optimization Experiments\n",
    "\n",
    "Test each pipeline configuration and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments run: 12\n",
      "\n",
      "Rankings by MRR:\n",
      "\n",
      "1. Config 3: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 1.0000, Precision@5: 0.2500, Recall@5: 1.2500\n",
      "\n",
      "2. Config 6: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 1.0000, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "3. Config 9: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 1.0000, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "4. Config 12: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 1.0000, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "5. Config 1: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2500, Recall@5: 1.2500\n",
      "\n",
      "6. Config 2: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2500, Recall@5: 1.2500\n",
      "\n",
      "7. Config 4: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "8. Config 5: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "9. Config 7: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "10. Config 8: SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "11. Config 10: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n",
      "12. Config 11: SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   MRR: 0.8750, Precision@5: 0.2000, Recall@5: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The optimization experiments are run in cell-6 above.\n",
    "# Here we can inspect the experiment results in more detail.\n",
    "\n",
    "print(f\"Total experiments run: {len(experiment_results)}\\n\")\n",
    "\n",
    "# Show all results sorted by MRR\n",
    "sorted_results = sorted(experiment_results, key=lambda x: x['metrics']['mrr'], reverse=True)\n",
    "\n",
    "print(\"Rankings by MRR:\\n\")\n",
    "for i, result in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {result['config_name']}\")\n",
    "    print(f\"   MRR: {result['metrics']['mrr']:.4f}, Precision@5: {result['metrics']['precision@5']:.4f}, Recall@5: {result['metrics']['recall@5']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Use Best Pipeline for Q&A\n",
    "\n",
    "Create a simple retrieval function using the best pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Retrieved Context:\n",
      "1. Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy.\n",
      "2. Chunking strategies such as sliding windows affect recall and precision in RAG pipelines.\n",
      "3. accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval using the best artifact directly (RAGArtifact is a BaseRetriever)\n",
    "test_query = \"What is RAG?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "docs = best_artifact.invoke(test_query)\n",
    "print(\"Retrieved Context:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1og0sm2h7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is RAG?\n",
      "A: Retrieval-Augmented Generation (RAG) is a method that combines information retrieval with text generation to enhance the factual accuracy of generated content. It leverages external information sources to provide more reliable and contextually relevant responses.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What do vector databases do?\n",
      "A: Vector databases store embeddings and enable efficient similarity search for retrieval systems.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Use the provided context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "qa_chain = create_retrieval_chain(best_artifact, doc_chain)\n",
    "\n",
    "sample_questions = [\n",
    "    eval_dataset[0]['query'],\n",
    "    eval_dataset[1]['query'],\n",
    "]\n",
    "\n",
    "for q in sample_questions:\n",
    "    out = qa_chain.invoke({\"input\": q})\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", out[\"answer\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Save Best Configuration\n",
    "\n",
    "Save the best pipeline configuration for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best configuration info to best_rag_config.json\n",
      "Saved best artifact to best_artifact.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save best configuration info using pickle\n",
    "import pickle\n",
    "\n",
    "best_info = {\n",
    "    'config_name': best_result['config_name'],\n",
    "    'metrics': best_result['metrics'],\n",
    "}\n",
    "\n",
    "with open('best_rag_config.json', 'w') as f:\n",
    "    json.dump(best_info, f, indent=2)\n",
    "print(\"Saved best configuration info to best_rag_config.json\")\n",
    "\n",
    "with open('best_artifact.pkl', 'wb') as f:\n",
    "    pickle.dump(best_artifact, f)\n",
    "print(\"Saved best artifact to best_artifact.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Example: Creating a Custom Pipeline\n",
    "\n",
    "Demonstrate how to create a pipeline with specific component instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Results:\n",
      "  1. {'text': 'Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy.', 'score': 6.015378475189209, 'chunk_id': 0, 'doc_id': 'doc_1', 'original_score': 0.3975445191976605, 'rerank_score': 6.015378475189209}\n",
      "  2. {'text': 'Chunking strategies such as sliding windows affect recall and precision in RAG pipelines.', 'score': -4.435868263244629, 'chunk_id': 0, 'doc_id': 'doc_3', 'original_score': 0.3627310530889852, 'rerank_score': -4.435868263244629}\n",
      "  3. {'text': 'Vector databases store embeddings and enable efficient similarity search for retrieval systems.', 'score': -11.362852096557617, 'chunk_id': 0, 'doc_id': 'doc_2', 'original_score': 0.03317414874458483, 'rerank_score': -11.362852096557617}\n",
      "  4. {'text': 'Embedding models map text into dense vector representations used for semantic search.', 'score': -11.3738431930542, 'chunk_id': 0, 'doc_id': 'doc_4', 'original_score': 0.034143145429370976, 'rerank_score': -11.3738431930542}\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a custom pipeline with specific component instances\n",
    "from rag_search.rag_client import RAGClient\n",
    "\n",
    "# Create custom components\n",
    "custom_chunking = SlidingWindowChunking(chunk_size=256, chunk_overlap=30)\n",
    "custom_embedding = HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "custom_storage = SimpleStorage()\n",
    "custom_retriever = SimpleRetriever(\n",
    "    storage=custom_storage,\n",
    "    embedding=custom_embedding,\n",
    "    top_k=5,\n",
    "    reranker=CrossEncoderReranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    ")\n",
    "\n",
    "# Build custom client\n",
    "custom_client = RAGClient(\n",
    "    ingestion=SimpleIngestion(),\n",
    "    chunking=custom_chunking,\n",
    "    embedding=custom_embedding,\n",
    "    storage=custom_storage,\n",
    "    retriever=custom_retriever,\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "custom_client.upload_documents(documents)\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is RAG?\"\n",
    "results = custom_client.retrieve(query)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
